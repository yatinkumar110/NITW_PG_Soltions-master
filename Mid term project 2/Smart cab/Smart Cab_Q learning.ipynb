{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "[2020-01-05 19:10:27,835] Making new env: Taxi-v2\nC:\\Users\\Hemanth\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n  result = entry_point.load(False)\n+---------+\n|R: | : :G|\n| : : : :|\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n\n"
    }
   ],
   "source": [
    "import gym\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "#for text processing\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Origing, Destination, and Time of Pickup from the sms data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pickup_drop(text):\n",
    "    \n",
    "    #Write your code here\n",
    "    time_of_pickup = []\n",
    "    locations_df = pd.read_csv('city.csv')\n",
    "    cities = list(locations_df['location'])\n",
    "    sms_cities = []\n",
    "    origin = []\n",
    "    dest = []\n",
    "    \n",
    "    # Extract the origin and destination location\n",
    "    for city in cities:\n",
    "        if city in text:\n",
    "            sms_cities.append(city)\n",
    "    \n",
    "    from_match = re.findall(r\"from\\s\\w+\\s\\w+\\s\\w+\" , text)\n",
    "    from_match = list(from_match)\n",
    "    \n",
    "    to_match = re.findall(r\"to\\s\\w+\\s\\w+\\s\\w+\" , text)\n",
    "    to_match = list(to_match)\n",
    "    \n",
    "    for city in sms_cities:\n",
    "        if (len(from_match)>0):\n",
    "            if(city in from_match[0]):\n",
    "                origin.append(city)\n",
    "                sms_cities.remove(city)\n",
    "                dest.append(sms_cities[0])\n",
    "        else:\n",
    "            if (len(to_match)>0):\n",
    "                if(city in to_match[0]):\n",
    "                    dest.append(city)\n",
    "                    sms_cities.remove(city)\n",
    "                    origin.append(sms_cities[0])\n",
    "    \n",
    "    # Extract the time from the text\n",
    "    time = re.findall(r\"(\\d+ PM)|(\\d+ AM)\",text)\n",
    "    if time[0][0] == '':\n",
    "        time_of_pickup = time[0][1]\n",
    "    else:\n",
    "        time_of_pickup = time[0][0]\n",
    "        \n",
    "    return [origin, dest, time_of_pickup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "+---------+\n|R:| : :G|\n| : : : : |\n| : : : : |\n| | : | : |\n|Y| : |B: |\n+---------+\n\nAction Space Discrete(6)\nState Space Discrete(500)\n"
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the Q-Learning Process\n",
    "Breaking it down into steps, we get\n",
    "\n",
    "Initialize the Q-table by all zeros.\n",
    "\n",
    "Start exploring actions: \n",
    "\n",
    "For each state, select any one among all possible actions for the current state (S).\n",
    "\n",
    "Travel to the next state (S') as a result of that action (a).\n",
    "\n",
    "For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "\n",
    "Update Q-table values using the equation.\n",
    "\n",
    "Set the next state as the current state.\n",
    "\n",
    "If goal state is reached, then end and repeat the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting learned values\n",
    "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called Ïµ \"epsilon\" to cater to this during training.\n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Q_table\n",
    "import numpy as np\n",
    "#write your code here\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Episode: 100000\nWall time: 58.9 s\n"
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "##Write your code here\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "        \n",
    "np.save(\"./q_table.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained q_table for evaluation\n",
    "\n",
    "q_table = np.load(\"./q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loc_dict(city_df):\n",
    "    loc_dict = {}\n",
    "    ## Create dictionary example, loc_dict['dwarka sector 23] = 0\n",
    "    for index ,row in city_df.iterrows():\n",
    "        loc_dict[row['location']] = row['mapping'] \n",
    "    return loc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pick_up_drop_correction(pick_up, drop, line_num):\n",
    "    #write your code here\n",
    "    orig_df = pd.read_csv('./org_df.csv')\n",
    "    original_origin = orig_df.iloc[line_num]['origin']\n",
    "    original_destination = orig_df.iloc[line_num]['dest']\n",
    "    if original_origin == pick_up and original_destination == drop:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Results after 1000 episodes:\nAverage timesteps per episode: 13.303\nAverage penalties per episode: 1.029\nTotal number of wrong predictions1029\nTotal Reward is10000\n"
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "# 1) We need to take text drom \"sms.txt\" and fetch pickup and drop from it.\n",
    "# 2) Generate the random state from an enviroment and change the pick up and drop as the fetched one\n",
    "# 3) Evaluate you q_table performance on all the texts given in sms.txt.\n",
    "# 4) Have a check if the fetched pickup, drop is not matching with original pickup, drop using orig.csv\n",
    "# 5) If fetched pickup or/and drop does not match with the original, add penality and reward -10\n",
    "# 6) Calculate the Total reward, penalities, Wrong pickup/drop predicted and Average time steps per episode.\n",
    "\n",
    "total_epochs, total_penalties, total_reward, wrong_predictions = 0, 0, 0, 0\n",
    "\n",
    "info = pd.DataFrame(columns=['origin','destination','time'])\n",
    "count = 0\n",
    "time_list = []\n",
    "f = open(\"./sms.txt\", \"r\")\n",
    "num_of_lines = 1000\n",
    "city = pd.read_csv(\"./city.csv\")\n",
    "\n",
    "loc_dict = create_loc_dict(city)\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    l = fetch_pickup_drop(line)\n",
    "    pick_up = l[0]\n",
    "    drop = l[1]\n",
    "    decision = check_pick_up_drop_correction(pick_up,drop,line_num)\n",
    "    if not decision:\n",
    "        total_penalties += 1\n",
    "        reward = -10\n",
    "        total_reward += reward\n",
    "        wrong_predictions += 1\n",
    "    pickUP_idx = loc_dict[pick_up[0]]\n",
    "    drop_idx = loc_dict[drop[0]]\n",
    "    act_state = env.reset()\n",
    "    taxi_row, taxi_col,pick_up ,drop  = env.decode(act_state)\n",
    "    state = env.encode(taxi_row,taxi_col,int(pickUP_idx),int(drop_idx))\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    total_reward += reward\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Results after {num_of_lines} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / num_of_lines}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / num_of_lines}\")\n",
    "print(f\"Total number of wrong predictions \", total_penalties)\n",
    "print(\"Total Reward is \", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
